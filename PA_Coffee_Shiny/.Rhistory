ikea <- read_csv("https://uwmadison.box.com/shared/static/iat31h1wjg7abhd2889cput7k264bdzd.csv")
ikea <- read.csv("https://uwmadison.box.com/shared/static/iat31h1wjg7abhd2889cput7k264bdzd.csv")
ikea
aggregate(width ~ depth, ikea, mean)
colnames(ikea)
aggregate(width ~ category, ikea, mean)
ikea$cat_avg_width <- aggregate(width ~ category, ikea, mean)
for(cat in ikea$category){}
for(cat in ikea$category){print(cat)}
aggregate(width ~ category, ikea, mean)
for(cat in ikea$category)
{
}
ikea[ikea$category=="TV"]$newcol = 5
ikea[ikea$category=="TV"]
ikea[ikea$category=="TV",]
ikea[ikea$category=="TV",]$newcol = 5
ikea
unique(ikea$category)
ikea[ikea$category=="TV "]
ikea[ikea$category=="TV ",]$newcol = 5
ikea[ikea$category=="TV ",]
ikea[ikea$category=="TV ",]$name
ikea[ikea$category=="TV ",]$newcol
ikea[ikea$category=="TV ",]$newcol = 5
ikea$newcol = 5
ikea[ikea$category=="TV ",]
ikea[ikea$category=="TV ",]$newcol = 6
ikea[ikea$category=="TV ",]$newcol = 6
ikea[ikea$category=="TV ",]
aggregate(width ~ category, ikea, mean)
aggregate(width ~ category, ikea, mean)$category
aggregate(width ~ category, ikea, mean)[category == "TV "]
aggregate(width ~ category, ikea, mean)[aggregate(width ~ category, ikea, mean)$category == "TV "]
aggregate(width ~ category, ikea, mean)[aggregate(width ~ category, ikea, mean)$category == "TV ",]
aggregate(width ~ category, ikea, mean)[aggregate(width ~ category, ikea, mean)$category == "TV ",]$width
ikea[ikea$category=="TV ",]$newcol = aggregate(width ~ category, ikea, mean)[aggregate(width ~ category, ikea, mean)$category == "TV ",]$width
ikea <- read.csv("https://uwmadison.box.com/shared/static/iat31h1wjg7abhd2889cput7k264bdzd.csv")
aggregate(width ~ category, ikea, mean)
cat_width <- aggregate(width ~ category, ikea, mean)
library(ggplot2)
ggplot(cat_width) + geom_bar(aes(category,width))
ggplot(cat_width) + geom_point(aes(category,width))
ggplot(cat_width) + geom_bar(aes(width))
ggplot(cat_width) + geom_point(aes(category,width))
ggplot(cat_width) + geom_point(aes(width,category))
ggplot(cat_width) + geom_col(aes(category,width))
library(ggplot2)
?facet_wrap
library(ggplot2)
library(shiny)
library(tidyverse)
library(sf)
install.packages('sf')
install.package('tmap')
install.packages('tmap')
library(tidyverse)
library(sf)
library(tmap)
trees <- read_csv("https://uwmadison.box.com/shared/static/t1mk6i4u5ks5bjxaw2c7soe2z8i75m2o.csv")
roads <- read_sf("https://uwmadison.box.com/shared/static/28y5003s1d0w9nqjnk9xme2n86xazuuj.geojson")
buildings <- read_sf("https://uwmadison.box.com/shared/static/qfmrp9srsoq0a7oj0e7xmgu5spojr33e.geojson")
ggplot(trees) +
geom_point(
aes(longitude, latitude, col = species_group),
alpha = 0.8, size = 0.9
) +
scale_color_brewer(palette = "Set3") +
coord_fixed()
tm_shape(roads) +
tm_lines()
tm_shape(roads)
tm_shape(roads) + tm_lines()
tm_shape(roads) +
tm_lines() +
tm_shape(buildings) +
tm_polygons()
buildings
buildings[1]
roads
colnames(roads)
roads[1]
roads[1,]
roads[1,]$geometry
roads[1]
roads[2]
roads
roads$geometry
buildings
buildings$geometry
buildings[1,]
allMSN<- read.csv(/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv)
allMSN<- read.csv(~/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv)
allMSN<- read.csv(Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv)
allMSN<- read.csv(Users/marwanlloyd/Desktop/UW-Madison/"Stat 605"/hw3/allMSN_test.csv)
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv")
allMSN
?read.csv
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv", header = FALSE)
allMSN
colnames(allMSN)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
allMSN.head()
head(allMSN)
?tapply
tapply(DepDelay,DayOfWeek,mean)
tapply(allMSN$DepDelay,allMSN$DayOfWeek,mean)
!is.na(allMSN$DepDelay)
allMSN[!is.na(allMSN$DepDelay)]
allMSN[!is.na(allMSN$DepDelay),]
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
mean_table<- tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
colnames(mean_table) <- c("Mo", "Tu", "We", "Th", "Fr", "Sa", "Su")
colnames(mean_table)
mean_table
data.frame(mean_table)
rev(data.frame(mean_table))
data.frame(mean_table)
data.frame(,mean_table)
t(data.frame(mean_table))
aggregate(DepDelay, DayofWeek, delayed_MSN, mean)
aggregate(DepDelay ~ DayofWeek, delayed_MSN, mean)
aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean)
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))
colnames(t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean)))
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))[1]
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))[1,2]
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))[2,]
data.frame(t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))[2,])
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))[2,]
mean_table
data.frame(mean_table)
t(data.frame(mean_table))
t(data.frame(mean_table))[1]
t(data.frame(mean_table))[,1]
t(data.frame(mean_table))[[2]]
t(data.frame(mean_table))[0,1]
t(data.frame(mean_table))[1,1]
t(data.frame(mean_table))[2,1]
t(data.frame(mean_table))[1,2]
tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 1]
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 1] <- "Mo"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 1]
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 1] <- "Mo"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 2] <- "Tu"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 3] <- "We"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 4] <- "Th"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 5] <- "Fr"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 6] <- "Sa"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 7] <- "Su"
tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv", header = FALSE)
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv", header = FALSE)
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
allMSN
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
allMSN
delayed_MSN
aggregate(DepDelay ~ DayofWeek, delayed_MSN, mean)
aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean)
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 1] <- "Mo"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 2] <- "Tu"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 3] <- "We"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 4] <- "Th"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 5] <- "Fr"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 6] <- "Sa"
delayed_MSN$DayOfWeek[delayed_MSN$DayOfWeek == 7] <- "Su"
t(aggregate(DepDelay ~ DayOfWeek, delayed_MSN, mean))
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
mean_table<- tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
mean_table
colnames(mean_table)
colnames(data.frame(mean_table))
data.frame(mean.table)
data.frame(mean_table)
t(data.frame(mean_table))
colnames(t(data.frame(mean_table)))
colnames(t(data.frame(mean_table))) <- c("Mo", "Tu", "We", "Th", "Fr","Sa", "Su")
mean_output <- t(data.frame(mean_table))
colnames(mean_output)
colnames(mean_output) <- c("Mo", "Tu", "We", "Th", "Fr","Sa", "Su")
mean_output
write.csv(mean_output, "delays.txt")
mean_output
mean_output[1:3]
mean_output[1:7]
mean_output_2 <- mean_output[1:7]
mean_output_2
data.frame(mean_output_2)
mean_output
mean_output[,1]
mean_output[1,1]
mean_output[2]
mean_output[0]
mean_output_2
mean_output
mean_table
t(data.frame(mean_table))
data.frame(mean_table)
tester <- data.frame(mean_table)
tester
names(tester)<- NULL
tester
t(tester)
mean_table<- tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
mean_output <- data.frame(mean_table)
names(mean_output) <- NULL
mean_output <- t(mean_output)
colnames(mean_output)
colnames(mean_output) <- c("Mo", "Tu", "We", "Th", "Fr","Sa", "Su")
mean_output
write.csv(mean_output, "delays.txt")
write.csv(mean_output, "delays.txt", row.names = FALSE)
mean_output
round(mean_output,2)
round(mean_output,1)
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN_test.csv", header = FALSE)
allMSN
#allMSN<- read.csv("allMSN.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
mean_table<- tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
mean_table
delayed_MSN
delayed_MSN[delayed_MSN$DayOfWeek > 3]
delayed_MSN[(delayed_MSN$DayOfWeek > 3) & (delayed_MSN$DayOfWeek < 5),]
delayed_MSN[(delayed_MSN$DayOfWeek > 0) & (delayed_MSN$DayOfWeek < 8),]
?sort
is.numeric(delayed_MSN$DepDelay)
is.numeric(delayed_MSN$DepDelay[10:20])
is.numeric(delayed_MSN$DepDelay[10:20],)
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/scripts/allMSN_2.csv", header = FALSE)
allMSN
#allMSN<- read.csv("allMSN.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
unique(allMSN$DayOfWeek)
delayed_MSN <- delayed_MSN[(delayed_MSN$DayOfWeek > 0) & (delayed_MSN$DayOfWeek < 8),]
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/scripts/allMSN_2.csv", header = FALSE)
#allMSN<- read.csv("allMSN.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
delayed_MSN <- delayed_MSN[!is.na(delayed_MSN$DayOfWeek),]
delayed_MSN <- delayed_MSN[(delayed_MSN$DayOfWeek > 0) & (delayed_MSN$DayOfWeek < 8),]
unique(delayed_MSN$DayOfWeek)
unique(delayed_MSN$DepDelay)
sort(unique(delayed_MSN$DepDelay))
tapply(delayed_MSN$DepDelay,delayed_MSN$DayOfWeek,mean)
tapply(as.numeric(delayed_MSN$DepDelay),delayed_MSN$DayOfWeek,mean)
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/allMSN.csv", header = FALSE)
#allMSN<- read.csv("allMSN.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
delayed_MSN <- delayed_MSN[!is.na(delayed_MSN$DayOfWeek),]
delayed_MSN <- delayed_MSN[(delayed_MSN$DayOfWeek > 0) & (delayed_MSN$DayOfWeek < 8),] #data cleaning bad rows
mean_table<- tapply(as.numeric(delayed_MSN$DepDelay),delayed_MSN$DayOfWeek,mean)
print(mean_table)
mean_output <- data.frame(mean_table)
names(mean_output) <- NULL
mean_output <- t(mean_output)
colnames(mean_output) <- c("Mo", "Tu", "We", "Th", "Fr","Sa", "Su")
rounded_mean_output <- round(mean_output,1)
rounded_mean_output
allMSN
delayed_MSN
mean(delayed_MSN$DepDelay)
allMSN<- read.csv("/Users/marwanlloyd/Desktop/UW-Madison/Stat 605/hw3/working_scripts/plane_scripts/allMSN.csv", header = FALSE)
#allMSN<- read.csv("allMSN.csv", header = FALSE)
colnames(allMSN) <- c("DayOfWeek", "DepDelay", "Origin", "Dest", "Distance")
delayed_MSN <- allMSN[!is.na(allMSN$DepDelay),]
delayed_MSN <- delayed_MSN[!is.na(delayed_MSN$DayOfWeek),]
delayed_MSN <- delayed_MSN[(delayed_MSN$DayOfWeek > 0) & (delayed_MSN$DayOfWeek < 8),] #data cleaning bad rows
mean_table<- tapply(as.numeric(delayed_MSN$DepDelay),delayed_MSN$DayOfWeek,mean)
print(mean_table)
mean_output <- data.frame(mean_table)
names(mean_output) <- NULL
mean_output <- t(mean_output)
colnames(mean_output) <- c("Mo", "Tu", "We", "Th", "Fr","Sa", "Su")
rounded_mean_output <- round(mean_output,1)
rounded_mean_output
?commandArgs
?stop
"data" + "/"
concat("data","/")
paste("data", "/")
paste("data","/")
paste("data","/", sep = "")
print("PATH:")
print("Path:" data_directory)
print("Path:", data_directory)
print(Path: data_directory)
print("Path")
print(c("Path:", data_directory))
tester <- 9+10
print(c("Path:", tester))
library(FITSio)
library(dplyr)
?readFrameFromFITS
install.packages("tidytext")
install.packages("topicmodels")
library(tidyverse)
library(tidytext)
library(topicmodels)
theme_set(theme_bw())
set.seed(1234)
stop_words
reviews <- read_csv("https://github.com/krisrs1128/stat479_s22/blob/main/_slides/week11/exercises/reviews.csv?raw=true") %>%
slice_sample(n = 2500)  # using half the data for convenience
reviews_dtm <- reviews %>%
unnest_tokens(word, Review) %>%
filter(!(word %in% stop_words$word)) %>%
count(document, word) %>%
cast_dtm(document, word, n)
tic <- Sys.time()
fit <- LDA(reviews_dtm, k = 8, control = list(seed = 1234))
Sys.time() - tic
fit@gamma
fit$gamma
?cast_dtm
?install.packages
len(c(1,2,3))
length(c(1,2,3))
?paste0
library(shiny)
library(tidyverse)
library(lubridate)
library(tidygraph)
library(ggraph)
library(ggplot2)
library(utils)
library(SentimentAnalysis)
library(SnowballC)
library(wordcloud)
library(tm)
library(stopwords)
coffee_shops <- read_csv("/Users/marwanlloyd/Github/Stat628_Module3_Group3/best_filtered_smaller_data.csv")
#Find all reviews that include bake in them
grepl("Bake",coffee_shops$text, fixed = FALSE, ignore.case = TRUE)
#Count reviews per state
table(coffee_shops$state)
#filter to only PA because that has the most
PA_shops <- coffee_shops[coffee_shops$state == "PA",]
#Get all PAs that aren't starbucks
PA_shops_2 <- PA_shops[!grepl("Starbuck", PA_shops$name_business_or_review, fixed = FALSE, ignore.case = TRUE),]
PA_shops$Sent_QDAP <- analyzeSentiment(PA_shops$text)$SentimentQDAP
PA_shops <- PA_shops[!is.na(PA_shops$Sent_QDAP),]
PA_shops$Sent_direct <- convertToDirection(PA_shops$Sent_QDAP)
PA_shops$Sent_resp <- 0
#PA_shops$Sent_resp[PA_shops$Sent_direct == "positive"] <- 1
#PA_shops$Sent_resp[PA_shops$Sent_direct == "negative"] <- -1
## Changing to handle adjusted scores
PA_shops$Sent_resp[PA_shops$Sent_QDAP < quantile(PA_shops$Sent_QDAP)[3] - .05] <- -1
PA_shops$Sent_resp[PA_shops$Sent_QDAP > quantile(PA_shops$Sent_QDAP)[3] + .05] <- 1
#Plot Sentiment Values
plotSentiment(PA_shops$Sent_QDAP)
ggplot(PA_shops)+
geom_point(aes(PA_shops$stars_business, PA_shops$Sent_QDAP)) +
labs(x = "Business Rating",
y = "Review Sentiment Score") + theme(plot.title = element_text(hjust = 0.5)) + labs(title = "Review Sentiment Spread Across Ratings")
avg_stars_and_sent <- aggregate(cbind(Sent_QDAP,stars_business) ~ name_business_or_review + address, PA_shops, mean)
#linear regression on the averages b/w stars and sentiment
summary(lm(stars_business ~ Sent_QDAP, avg_stars_and_sent))
#linear regression when not averaged out
summary(lm(stars_business ~ Sent_QDAP, PA_shops))
plot(avg_stars_and_sent$Sent_QDAP, avg_stars_and_sent$stars_business,
main = "Avg Sentiment vs. Business Star Rating",
xlab = "Avg Sentiment",
ylab = "Star Rating")
abline(lm(stars_business ~ Sent_QDAP, avg_stars_and_sent), col = "red")
#Generating Word Clouds setup
positive_resp_texts <- PA_shops[PA_shops$Sent_resp == 1,]$text
negative_resp_texts <- PA_shops[PA_shops$Sent_resp == -1,]$text
positive_docs <- Corpus(VectorSource(positive_resp_texts))
negative_docs <- Corpus(VectorSource(negative_resp_texts))
#Cleaning positive docs
positive_docs <- positive_docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
positive_docs <- tm_map(positive_docs, content_transformer(tolower))
positive_docs <- tm_map(positive_docs, removeWords, stopwords("english"))
#Cleaning negative docs
negative_docs <- negative_docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
negative_docs <- tm_map(negative_docs, content_transformer(tolower))
negative_docs <- tm_map(negative_docs, removeWords, stopwords("english"))
positive_dtm <- TermDocumentMatrix(positive_docs)
negative_dtm <- TermDocumentMatrix(negative_docs)
positive_matrix <- as.matrix(positive_dtm)
negative_matrix <- as.matrix(negative_dtm)
positive_words <- sort(rowSums(positive_matrix),decreasing=TRUE)
negative_words <- sort(rowSums(negative_matrix),decreasing=TRUE)
positive_df <- data.frame(word = names(positive_words),freq=positive_words)
negative_df <- data.frame(word = names(negative_words),freq=negative_words)
#filter out stop words
filtered_positive_df <- positive_df %>%
filter(!(word %in% stopwords(source = "smart")))
filtered_negative_df <- negative_df %>%
filter(!(word %in% stopwords(source = "smart")))
filtered_negative_df
#Add additional filtering down to keywords
extra_removal_words <- c("starbucks", "ive", "didnt", "dont", "one", "can", "get", "just", "also", "know", "great", "good", "love", "make", "like", "place", "really", "even", "always", "best", "well", "coffee")
filtered_positive_df <- filtered_positive_df %>%
filter(!(word %in% extra_removal_words))
filtered_negative_df <- filtered_negative_df %>%
filter(!(word %in% extra_removal_words))
filtered_negative_df
shiny::runApp('Github/Stat628_Module3_Group3/PA_Coffee_Shiny')
top_words
library(shiny)
library(tidyverse)
library(tidygraph)
library(ggraph)
library(ggplot2)
library(utils)
library(SentimentAnalysis)
library(SnowballC)
library(wordcloud)
library(tm)
library(stopwords)
library(shinythemes)
library(leaflet)
library(tidytext)
library(stringr)
library(bipartite)
library(tidytext)
library(DT)
#coffee_shops <- read_csv("/Users/marwanlloyd/Github/Stat628_Module3_Group3/best_filtered_smaller_data.csv")
PA_shops <- read_csv("PA_Coffee_Shiny/Shiny_Data/PA_Yelp_Coffee_Data.csv")
PA_shops$Sent_QDAP <- analyzeSentiment(PA_shops$text)$SentimentQDAP
PA_shops <- PA_shops[!is.na(PA_shops$Sent_QDAP),]
PA_shops$Sent_direct <- convertToDirection(PA_shops$Sent_QDAP)
PA_shops$Sent_resp <- 0
#PA_shops$Sent_resp[PA_shops$Sent_direct == "positive"] <- 1
#PA_shops$Sent_resp[PA_shops$Sent_direct == "negative"] <- -1
## Changing to handle adjusted scores
PA_shops$Sent_resp[PA_shops$Sent_QDAP < quantile(PA_shops$Sent_QDAP)[3] - .05] <- -1
PA_shops$Sent_resp[PA_shops$Sent_QDAP > quantile(PA_shops$Sent_QDAP)[3] + .05] <- 1
avg_stars_and_sent <- aggregate(cbind(Sent_QDAP,stars_business) ~ name_business_or_review + address, PA_shops, mean)
extra_removal_words <- c("starbucks", "ive", "didnt", "dont", "one", "can", "get", "just", "also", "know", "great", "good", "love", "make", "like", "place", "really", "even", "always", "best", "well", "coffee", "tea", "latte", "cappucino", "cappuccino", "bad", "poor", "get", "best", "just", "friendly", "nice", "back", "pretty", "super", "clean", "location", "day", "work", "perfect", "city", "town","neighborhood", "philly", "open", "horrible", "terrible", "disappointed", "woman", "extremely", "order", "amazing", "ordered", "recommend", "enjoy", "awesome", "excellent", "times", "short", "coming", "worse", "hate")
positive_resp_texts <- PA_shops[PA_shops$Sent_resp == 1,]$text
negative_resp_texts <- PA_shops[PA_shops$Sent_resp == -1,]$text
positive_docs <- Corpus(VectorSource(positive_resp_texts))
negative_docs <- Corpus(VectorSource(negative_resp_texts))
positive_docs <- positive_docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
positive_docs <- tm_map(positive_docs, content_transformer(tolower))
positive_docs <- tm_map(positive_docs, removeWords, stopwords("english"))
negative_docs <- negative_docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
negative_docs <- tm_map(negative_docs, content_transformer(tolower))
negative_docs <- tm_map(negative_docs, removeWords, stopwords("english"))
positive_dtm <- TermDocumentMatrix(positive_docs)
negative_dtm <- TermDocumentMatrix(negative_docs)
positive_matrix <- as.matrix(positive_dtm)
negative_matrix <- as.matrix(negative_dtm)
positive_words <- sort(rowSums(positive_matrix),decreasing=TRUE)
negative_words <- sort(rowSums(negative_matrix),decreasing=TRUE)
positive_df <- data.frame(word = names(positive_words),freq=positive_words)
negative_df <- data.frame(word = names(negative_words),freq=negative_words)
#filter out stop words
filtered_positive_df <- positive_df %>%
filter(!(word %in% stopwords(source = "smart")))
filtered_negative_df <- negative_df %>%
filter(!(word %in% stopwords(source = "smart")))
filtered_positive_df <- filtered_positive_df %>%
filter(!(word %in% extra_removal_words))
filtered_negative_df <- filtered_negative_df %>%
filter(!(word %in% extra_removal_words))
top_positive_words <- head(filtered_positive_df[analyzeSentiment(filtered_positive_df$word)$SentimentQDAP == 1,], n = 25)
top_negative_words <- head(filtered_negative_df[analyzeSentiment(filtered_negative_df$word)$SentimentQDAP == -1,], n = 26)
top_negative_words <- top_negative_words[!is.na(top_negative_words$word),]
top_words <- as.data.frame(cbind(top_positive_words$word, top_negative_words$word))
colnames(top_words) <- c('Positives', 'Negatives')
top_words
as.data.frame(top_words)
write.csv(as.data.frame(top_words), "top_words.csv")
getwd()
setwd("/Users/marwanlloyd/Github/Stat628_Module3_Group3/PA_Coffee_Shiny")
read_csv("Shiny_Data/top_words.csv")
as.data.frame(read_csv("Shiny_Data/top_words.csv"))
write.csv(as.data.frame(top_words), "top_words.csv", row.names = FALSE)
as.data.frame(read_csv("Shiny_Data/top_words.csv"))
read_csv("Shiny_Data/top_words.csv")
top_words <- read_csv("Shiny_Data/top_words.csv")
top_words
top_words <- as.data.frame(read_csv("Shiny_Data/top_words.csv"))
top_words
runApp()
runApp()
PA_shops
colnames(PA_shops)
write.csv(PA_shops,"PA_Yelp_Coffee_Data.csv", row.names = FALSE)
runApp()
rsconnect::deployApp('/Users/marwanlloyd/Github/Stat628_Module3_Group3/PA_Coffee_Shiny')
rsconnect::deployApp('/Users/marwanlloyd/Github/Stat628_Module3_Group3/PA_Coffee_Shiny')
rsconnect::deployApp('/Users/marwanlloyd/Github/Stat628_Module3_Group3/PA_Coffee_Shiny')
