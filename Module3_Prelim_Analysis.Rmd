---
title: "Module3_Prelim"
author: "Marwan Lloyd"
date: '2022-11-17'
output: html_document
---

```{r setup, include=FALSE}
library(shiny)
library(tidyverse)
library(lubridate)
library(tidygraph)
library(ggraph)
library(ggplot2)
library(utils)
library(SentimentAnalysis)
library(SnowballC)
library(wordcloud)
library(tm)
library(stopwords)
```


```{r}
coffee_shops <- read_csv("/Users/marwanlloyd/Github/Stat628_Module3_Group3/best_filtered_smaller_data.csv")
```

```{r}
#Find all reviews that include bake in them 
grepl("Bake",coffee_shops$text, fixed = FALSE, ignore.case = TRUE)
```

```{r}
#Count reviews per state 

table(coffee_shops$state)

#filter to only PA because that has the most 

PA_shops <- coffee_shops[coffee_shops$state == "PA",]

#Get all PAs that aren't starbucks 
PA_shops_2 <- PA_shops[!grepl("Starbuck", PA_shops$name_business_or_review, fixed = FALSE, ignore.case = TRUE),]
```

```{r}
PA_shops$Sent_QDAP <- analyzeSentiment(PA_shops$text)$SentimentQDAP
PA_shops <- PA_shops[!is.na(PA_shops$Sent_QDAP),]
PA_shops$Sent_direct <- convertToDirection(PA_shops$Sent_QDAP)
PA_shops$Sent_resp <- 0
#PA_shops$Sent_resp[PA_shops$Sent_direct == "positive"] <- 1
#PA_shops$Sent_resp[PA_shops$Sent_direct == "negative"] <- -1
## Changing to handle adjusted scores 
PA_shops$Sent_resp[PA_shops$Sent_QDAP < quantile(PA_shops$Sent_QDAP)[3] - .05] <- -1
PA_shops$Sent_resp[PA_shops$Sent_QDAP > quantile(PA_shops$Sent_QDAP)[3] + .05] <- 1

```


```{r}
#Plot Sentiment Values
plotSentiment(PA_shops$Sent_QDAP)
```

```{r}
ggplot(PA_shops)+
  geom_point(aes(PA_shops$stars_business, PA_shops$Sent_QDAP)) +
  labs(x = "Business Rating",
       y = "Review Sentiment Score") + theme(plot.title = element_text(hjust = 0.5)) + labs(title = "Review Sentiment Spread Across Ratings")
```


```{r}
avg_stars_and_sent <- aggregate(cbind(Sent_QDAP,stars_business) ~ name_business_or_review + address, PA_shops, mean)
```

```{r}
#linear regression on the averages b/w stars and sentiment
summary(lm(stars_business ~ Sent_QDAP, avg_stars_and_sent))
```

```{r}
#linear regression when not averaged out
summary(lm(stars_business ~ Sent_QDAP, PA_shops))
```

The big takeaway b/w the 2 linear regressions done above, is that 1 individual review sentiment may not be a major predictor for star rating, but the average of them are. So that's why it's important to listen to ways to improve reviews. 

```{r}
plot(avg_stars_and_sent$Sent_QDAP, avg_stars_and_sent$stars_business,
     main = "Avg Sentiment vs. Business Star Rating",
     xlab = "Avg Sentiment",
     ylab = "Star Rating")
abline(lm(stars_business ~ Sent_QDAP, avg_stars_and_sent), col = "red")
```


```{r}
#Generating Word Clouds setup

positive_resp_texts <- PA_shops[PA_shops$Sent_resp == 1,]$text
negative_resp_texts <- PA_shops[PA_shops$Sent_resp == -1,]$text
positive_docs <- Corpus(VectorSource(positive_resp_texts))
negative_docs <- Corpus(VectorSource(negative_resp_texts))
```

```{r}
#Cleaning positive docs
positive_docs <- positive_docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
positive_docs <- tm_map(positive_docs, content_transformer(tolower))
positive_docs <- tm_map(positive_docs, removeWords, stopwords("english"))
```

```{r}
#Cleaning negative docs
negative_docs <- negative_docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
negative_docs <- tm_map(negative_docs, content_transformer(tolower))
negative_docs <- tm_map(negative_docs, removeWords, stopwords("english"))
```

```{r}
positive_dtm <- TermDocumentMatrix(positive_docs) 
negative_dtm <- TermDocumentMatrix(negative_docs)
positive_matrix <- as.matrix(positive_dtm)
negative_matrix <- as.matrix(negative_dtm)
positive_words <- sort(rowSums(positive_matrix),decreasing=TRUE)
negative_words <- sort(rowSums(negative_matrix),decreasing=TRUE)
positive_df <- data.frame(word = names(positive_words),freq=positive_words)
negative_df <- data.frame(word = names(negative_words),freq=negative_words)

```

```{r}
#filter out stop words 
filtered_positive_df <- positive_df %>% 
  filter(!(word %in% stopwords(source = "smart")))

filtered_negative_df <- negative_df %>% 
  filter(!(word %in% stopwords(source = "smart")))
```

```{r}
#Add additional filtering down to keywords
extra_removal_words <- c("starbucks", "ive", "didnt", "dont", "one", "can", "get", "just", "also", "know", "great", "good", "love", "make", "like", "place", "really", "even", "always", "best", "well", "coffee")
filtered_positive_df <- filtered_positive_df %>% 
  filter(!(word %in% extra_removal_words))

filtered_negative_df <- filtered_negative_df %>% 
  filter(!(word %in% extra_removal_words))
```

```{r}
#Generating Positive Word Cloud
wordcloud(words = filtered_positive_df$word, freq = filtered_positive_df$freq, min.freq = 1,           max.words=20, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

```{r}
#generate negative word cloud 
wordcloud(words = filtered_negative_df$word, freq = filtered_negative_df$freq, min.freq = 1,           max.words=20, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```


```{r}
#generate positive word cloud of positive exclusive words 
wordcloud(words = filtered_positive_df[!(filtered_positive_df$word %in% head(filtered_negative_df$word, n = 30)),]$word, freq = filtered_positive_df[!(filtered_positive_df$word %in% head(filtered_negative_df$word, n = 30)),]$freq, min.freq = 1,           max.words=20, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```


```{r}
#generate negative word cloud of negative exclusive words 
wordcloud(words = filtered_negative_df[!(filtered_negative_df$word %in% head(filtered_positive_df$word, n = 30)),]$word, freq = filtered_negative_df[!(filtered_negative_df$word %in% head(filtered_positive_df$word, n = 30)),]$freq, min.freq = 1,           max.words=20, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

